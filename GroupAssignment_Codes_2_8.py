# -*- coding: utf-8 -*-
"""GroupAssignment_Codes_2_8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLt1rh7EDOFyIHrj3orV6QDGCPNGn8jj

# Import libraries
"""

#Import Necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import cross_val_predict, StratifiedKFold, train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, balanced_accuracy_score
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, silhouette_samples
from sklearn.neighbors import LocalOutlierFactor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.manifold import TSNE

"""# Import Dataset"""

# Import dataset excel
df = pd.read_excel('Accidental_Drug_Related_Deaths_Cleaned.xlsx',keep_default_na=False)

"""# Pre_Processing"""

#convert "Other_Opioid" and "Other" to binary
df['Other_Opioid'] = df['Other_Opioid'].apply(lambda x: 1 if isinstance(x, str) and x.strip() != "None" else 0)
df['Other'] = df['Other'].apply(lambda x: 1 if isinstance(x, str) and x.strip() != "None" else 0)

print(df.head())

# Encoding features needed
le = LabelEncoder()
df["Sex"] = le.fit_transform(df["Sex"])
df["Race"] = le.fit_transform(df["Race"])
df["Ethnicity"] = le.fit_transform(df["Ethnicity"])
df['Age_Group'] = le.fit_transform(df['Age_Group'])
df['Cause_of_Death'] = le.fit_transform(df['Cause_of_Death'])
df['Other_Significant_Conditions_'] = le.fit_transform(df['Other_Significant_Conditions_'])
df['Description_of_Injury'] = le.fit_transform(df['Description_of_Injury'])

#encoding without changing dataset value
Death_City = le.fit_transform(df['Death_City'])
Injury_Place = le.fit_transform(df['Injury_Place'])
Injury_City = le.fit_transform(df['Injury_City'])
Injury_County = le.fit_transform(df['Injury_County'])
Injury_State = le.fit_transform(df['Injury_State'])

# Drop all redundant columns
df.copy
df.drop(columns=["Date", "Date_Type", "Residence_City", "Residence_County", "Residence_State", "Location", "Location_if_Other",
                 "Manner_of_Death", "Heroin_death_certificate_(DC)", "Any_Opioid", "Drug_Count", "Age_Group"], inplace=True)

print(df.head())

#export dataset
df.to_excel('Accidental_Drug_Related_Deaths_Cleaned_Preprocessed.xlsx', index=False)

"""# Classification"""

# Feature and Target selection
X = df[["Age", "Sex", "Race", "Ethnicity", "Cause_of_Death", "Description_of_Injury", "Other_Significant_Conditions_"]]
y = df["Fentanyl"]

# Dataset splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit RandomForestClassifier with GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_split': [5, 10, 15],
    'max_features': ['sqrt', 'log2'],
    'class_weight': ['balanced', 'balanced_subsample']
}

clf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='balanced_accuracy', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# Best model from grid search
best_clf = grid_search.best_estimator_

# Predictions
y_pred = best_clf.predict(X_test_scaled)
y_pred_proba = best_clf.predict_proba(X_test_scaled)[:, 1]

# Performance evaluation
print("RandomForestClassifier Performance Metrics:")

print("\nBest Hyperparameters:")
print(grid_search.best_params_)

print("\nCross Validation Performance:")
scores = cross_val_score(best_clf, X_train_scaled, y_train, cv=5, scoring='balanced_accuracy')
print("Cross-Validation Balanced Accuracy Scores:", scores)
print("Mean CV Balanced Accuracy:", scores.mean())

print("\nTest Set Performance:")
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Balanced Accuracy:", balanced_accuracy_score(y_test, y_pred))

print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

print("\nROC AUC Performance:")
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
auc_score = roc_auc_score(y_test, y_pred_proba)
print("False Positive Rate (FPR):", fpr)
print("True Positive Rate (TPR):", tpr)
print("ROC AUC Score:", auc_score)

print("\nPredicted Class: ", y_pred)

# Visualization: Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Visualization: ROC and AUC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Visualization: feature importance
importances = best_clf.feature_importances_
sorted_indices = np.argsort(importances)[::-1]

plt.figure(figsize=(8, 5))
plt.barh(X.columns[sorted_indices], importances[sorted_indices], color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()

"""# Clustering"""

# Feature selection
# List of drug types
drug_types = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone',
    'Oxymorphone', 'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone',
    'Meth/Amphetamine', 'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)',
    'Xylazine', 'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other'
]

# Select features
X = df[drug_types]

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Visualization: Dendogram
linked = linkage(X_scaled, method='complete')

plt.figure(figsize=(10, 7))
dendrogram(linked, truncate_mode='level', p=5)
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

# Agglomerative clustering
agg_clustering = AgglomerativeClustering(n_clusters=6, linkage='complete')
agg_labels = agg_clustering.fit_predict(X_scaled)

# Performance evaluation
sil_score = silhouette_score(X_scaled, agg_labels)
db_index = davies_bouldin_score(X_scaled, agg_labels)
ch_score = calinski_harabasz_score(X_scaled, agg_labels)

print("Agglomerative Clustering Performance Metrics:")
print(f"Detected Clusters: {len(set(agg_labels))}")
print(f"Silhouette Score: {sil_score:.3f}")
print(f"Davies-Bouldin Index: {db_index:.3f}")
print(f"Calinski-Harabasz Index: {ch_score:.3f}")

# Visualization: Silhouette Score Plot
plt.figure(figsize=(8, 5))
silhouette_values = silhouette_samples(X_scaled, agg_labels)
plt.bar(range(len(silhouette_values)), silhouette_values, color='skyblue')
plt.axhline(y=sil_score, color='red', linestyle='--', label='Average Silhouette Score')
plt.xlabel("Data Points")
plt.ylabel("Silhouette Coefficient Values")
plt.title("Silhouette Score Plot")
plt.legend(loc='upper right')
plt.show()

# Visualization: t-SNE
cluster_colors = {0: "purple", 1: "teal", 2: "yellow", 3: "blue", 4: "green", 5: "orange"}

tsne = TSNE(n_components=2, random_state=42)
data_2d = tsne.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
plt.title('t-SNE Visualization of Final Clusters')

for cluster in set(agg_labels):
    subset = data_2d[agg_labels == cluster]
    plt.scatter(subset[:, 0], subset[:, 1], c=cluster_colors.get(cluster, 'gray'), label=f'Cluster {cluster}', alpha=0.6)

plt.legend()
plt.show()

"""# Anomaly Detection

"""

# Feature selection
location_features = ['Death_City', 'Death_County', 'Death_State']

# Group by location and count deaths
death_counts = df.groupby(location_features).size().reset_index(name='Death_Count')

# Fit LOF model
lof = LocalOutlierFactor(n_neighbors=35, contamination=0.05)
death_counts['LOF_Score'] = lof.fit_predict(death_counts[['Death_Count']])

# Print outlier locations and their counts
anomalies = death_counts[death_counts['LOF_Score'] == -1]

print("Detected Outliers (Anomalies):")
print(anomalies[['Death_City', 'Death_County', 'Death_State', 'Death_Count']])

# Visualization: Combination of Scatter Plot and Box Plot
plt.figure(figsize=(12, 6))
sns.boxplot(data=death_counts, x='Death_County', y='Death_Count', hue='Death_County', palette='coolwarm', legend=False)
sns.scatterplot(data=anomalies, x='Death_County', y='Death_Count',
                color='red', marker='X', s=100, label='Outliers')

plt.xticks(rotation=90)
plt.title('Box Plot of Death Counts by County with Anomalies')
plt.xlabel('Death County')
plt.ylabel('Death Count')
plt.legend()
plt.tight_layout()
plt.show()

"""# Regression"""

# Feature Selection
X = np.column_stack([Injury_City, Injury_County, Injury_State, Injury_Place])
y = Death_City

# Data Spliiting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit RandomForestRegressor model with GridSearchCV
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# Best model from GridSearchCV
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test_scaled)

# Performance evaluation
# Convert negative MSE to positive for interpretation
scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
mse_scores = -scores
mean_mse = mse_scores.mean()

print("\nCross Validation Performance:")
print("Cross-Validation MSE Scores:", mse_scores)
print("Mean Cross-Validation MSE:", mean_mse)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

mean_target = np.mean(y_test)  # Mean of the true target values
rmae = mae / mean_target

print("\nBest Hyperparameters:")
print(f'Best Hyperparameters: {grid_search.best_params_}')

print("\nRandomForestRegressor Performance Metrics:")
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Relative Mean Absolute Error (RMAE): {rmae:.2%}')
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'R-squared (R²): {r2:.2f}')

# Visualization: Scatter Plot for Actual vs Predicted values
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='blue', label='Actual vs Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2, label='Perfect Fit')
plt.xlabel('Actual Death City')
plt.ylabel('Predicted Death City')
plt.title('RandomForestRegressor: Actual vs Predicted')
plt.legend()
plt.grid(True)
plt.show()

# Visualization: Feature Importance
features = ['Injury_City', 'Injury_County', 'Injury_State', 'Injury_Place']
importance_values = best_model.feature_importances_
sorted_indices = np.argsort(importance_values)[::-1]

plt.bar(np.array(features)[sorted_indices], importance_values[sorted_indices], color='skyblue')
plt.title('Feature Importance ')
plt.ylabel('Importance')
plt.show()