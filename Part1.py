# -*- coding: utf-8 -*-
"""AssignmentPart1_Code_2_0369793.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7zpYnJ2iw56gSYlFuDQ7NE7LCv57pz5

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict

"""# Import Dataset"""

#import datasetv Accidental_Drug_Related_Deaths.csv
df = pd.read_csv('Accidental_Drug_Related_Deaths.csv')

"""# Understanding Dataset"""

#dataset information
print("Dataset Overview")
print("\nNumber of Rows:", df.shape[0])
print("\nNumber of Columns:", df.shape[1])
print("\nColumn Names and Data Types:\n", df.dtypes)

#dataset summary
print("\nDataset Summary:\n",df.describe())

#dataset information
print("\nDataset Information:\n",df.info())

#the timeline of the dataset
print("Earliest date:", df["Date"].min())
print("Latest date:", df["Date"].max())

#sample record of dataset in the first 5
print("Sample Records of first 5:\n",df.head())

#sample record of dataset in the last 5
print("Sample Records of Last 5:\n",df.tail(5))

#unique values of all columns
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values for column '{column}':")
    print(unique_values)
    print()

#number of unique value in each column
# Number of unique values in each column
for column in df.columns:
    num_unique_values = df[column].nunique()
    print(f"Unique values for column '{column}': {num_unique_values}")

"""# Data Cleaning"""

#standardize naming of sex
print("Unique value (before): ", df['Sex'].unique())

sex = {
    'Unknown': 'Other',
    'X': 'Other'
}

df['Sex'] = df['Sex'].replace(sex)

print("\nUnique value (after): ", df['Sex'].unique())

#standardize the name of races
print("Unique value (before): ", df['Race'].unique())

race = {
    'Unknown': 'Other',
    'Asian, Other': 'Asian',
    'Other Asian': 'Asian',
    'Native American, Other': 'Native American',
    'Other Asian (Specify)': 'Asian',
    'Other (Specify)': 'Other',
    'white': 'White',
    'Asian/Indian': 'Asian Indian',
    'Other (Specify) Haitian': 'Haitian',
    'Other (Specify) portugese, Cape Verdean':'Portugese',
    'Other (Specify) Puerto Rican': 'Puerto Rican',
}

df['Race'] = df['Race'].replace(race)

print("\nUnique value (after): ", df['Race'].unique())

#standardize naming of ethinicity
print("\nUnique value (before) :", df['Ethnicity'].unique())

ethnicity = {
    'Other Spanish/Hispanic/Latino': 'Hispanic',
    'Spanish/Hispanic/Latino': 'Hispanic',
    'No, not Spanish/Hispanic/Latino': 'Non-Hispanic',
    'Not Spanish/Hispanic/Latino': 'Non-Hispanic',
    'Unknown': 'Other',
    'Yes, other Spanish/Hispanic/Latino': 'Hispanic',
    'Yes, Other Spanish/Hispanic/Latino (Specify)': 'Hispanic',
    'Yes, Puerto Rican': 'Puerto Rican',
    'Mexican, Mexican American, Chicano': 'Mexican',
    'Yes, Mexican, Mexican American, Chicano': 'Mexican',
}

df['Ethnicity'] = df['Ethnicity'].replace(ethnicity)

print("\nUnique value (after) :", df['Ethnicity'].unique())

#standardize the naming in residnece city
print("\nUnique value (before): ", df['Residence City'].unique())

residence_city = {
    "W HAVEN": "WEST HAVEN",
    "NO HAVEN": "NORTH HAVEN",
    "WILLIAMNTIC": "WILLIMANTIC",
    "COVENTRY CT 06238": "COVENTRY",
    "NEW LONDON CT": "NEW LONDON",
    "S GLASTONBURY": "SOUTH GLASTONBURY",
    "WEATHERSFIELD": "WETHERSFIELD"
}

df['Residence City'] = df['Residence City'].replace(residence_city)

print("\nUnique value (after): ", df['Residence City'].unique())

#standized naming of Injury State
print("\nUnique value (before): ", df['Injury State'].unique())

injury_state = {
    'CONNECTICUT': 'CT',
    'MASSACHUSSETS': 'MA',
    'UNKNOWN': 'Other',
    '':None
}

df['Injury State'] = df['Injury State'].replace(injury_state)

print("\nUnique value (after): ", df['Injury State'].unique())

#standardize naming of residence state
print("\nUnique value (before): ", df['Residence State'].unique())

df['Residence State'] = df['Residence State'].replace({'': None})

print("\nUnique value (after): ", df['Residence State'].unique())

#standardize name of injury city
print("\nUnique value (before ): ", df['Injury City'].unique())

injury_city = {
    "WILLIAMNTIC": "WILLIMANTIC",
    "BAKERSVILE": "BAKERSVILLE",
    "PRSOPECT": "PROSPECT",
    "COSCOB": "COS COB",
    "PORTCHESTER": "PORT CHESTER",
    "NO HAVEN": "NORTH HAVEN",
    "DANILESON": "DANIELSON",
    "NORTH WINDAM": "NORTH WINDHAM",
    "WATERBURY, CT": "WATERBURY",
    "COVENTRY CT 06238": "COVENTRY",
    "STAFFORD SPGS": "STAFFORD SPRINGS",
    "EASTHAVEN": "EAST HAVEN",
    "NEW LONDON CT": "NEW LONDON",
    "JEWITT CITY": "JEWETT CITY",
    "STORRS MANSFIELD": "STORRS"
}

df["Injury City"] = df["Injury City"].replace(injury_city)

print("\nUnique value (after): ", df['Injury City'].unique())

#standardize naming of Injury County
print("\nUnique values (before): ", df['Injury County'].unique())
injury_county = {
    '': None,
    'UNKNOWN': 'Other',
    'MNEW LONDON': 'NEW LONDON'
}

df['Injury County'] = df['Injury County'].replace(injury_county)

print("\nUnique values (after): ", df['Injury County'].unique())

#standardize name of death city
print("\nUnique values (before): ", df['Death City'].unique())

death_city = {
    "WILLIAMNTIC": "WILLIMANTIC",
    "N HAVEN": "NORTH HAVEN",
    "NO HAVEN": "NORTH HAVEN",
    "STAFFORD SPGS": "STAFFORD SPRINGS",
    "EASTHAVEN": "EAST HAVEN",
    "NEW LONDON CT": "NEW LONDON",
    "NORTH WINDAM": "NORTH WINDHAM",
    "JEWITT CITY": "JEWETT CITY"
}

df["Death City"] = df["Death City"].replace(death_city)

print("\nUnique values (after): ", df['Death City'].unique())

#standardize name of location
print("\nUnique values (before): ", df['Location'].unique())

location = {
    'Other (Specify)': 'Other',
    'Hiospital': 'Hospital'
}

df['Location'] = df['Location'].replace(location)

print("\nUnique values (after): ", df['Location'].unique())

#standardize name of manner of death
print("\nUnique values (before): ", df['Manner of Death'].unique())

manner_of_death = {
    'accident': 'Accident',
    'ACCIDENT': 'Accident',
    'Acciddent': 'Accident'
}

df['Manner of Death'] = df['Manner of Death'].replace(manner_of_death)

print("\nUnique values (after): ", df['Manner of Death'].unique())

#standardize naming of fentanyl
print("\nUnique value (before): ", df['Fentanyl'].unique())

fentanyl = {
    'Y POPS': 'Y',
    'Y (PTCH)': 'Y'
}

df['Fentanyl'] = df['Fentanyl'].replace(fentanyl)

print("\nUnique value (after): ", df['Fentanyl'].unique())

#standardized the name in morphine
print("\nUnique values(before): ", df['Morphine (Not Heroin)'].unique())

morphine = {
    'NO RX BUT STRAWS': None,
    'STOLE MEDS': 'Y',
    'PCP NEG': None
}

df['Morphine (Not Heroin)'] = df['Morphine (Not Heroin)'].replace(morphine)

print("\nUnique values(after): ", df['Morphine (Not Heroin)'].unique())

#standardize naming in gabapentin
print("\nUnique values (before): ", df['Gabapentin'].unique())

df['Gabapentin'] = df['Gabapentin'].replace({'y': 'Y'})

print("\nUnique values (after): ", df['Gabapentin'].unique())

#standardize naming in Heroin/Morph/Codeine
print("\n Unique values (after): ", df['Heroin/Morph/Codeine'].unique())

df['Heroin/Morph/Codeine'] = df['Heroin/Morph/Codeine'].replace({'y': 'Y'})

print("\n Unique values (after): ", df['Heroin/Morph/Codeine'].unique())

#handle missing values
print("Initial shape:\n", df.shape)
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value with mean
df['Age']=df['Age'].fillna(df['Age'].mean())

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value of ethinicity with mode
df['Ethnicity']=df['Ethnicity'].fillna(df['Ethnicity'].mode()[0])
print("\nMode of Ethnicity:", df["Ethnicity"].mode()[0])

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing values of drugs positive
columns_to_fill = [
    "Heroin", "Heroin death certificate (DC)", "Cocaine",
    "Fentanyl", "Fentanyl Analogue", "Oxycodone", "Oxymorphone", "Ethanol", "Hydrocodone",
    "Benzodiazepine", "Methadone", "Meth/Amphetamine", "Amphet", "Tramad", "Hydromorphone",
    "Morphine (Not Heroin)", "Xylazine", "Gabapentin", "Opiate NOS", "Heroin/Morph/Codeine",
    "Any Opioid"
]

#filling missing values with ""
df[columns_to_fill] = df[columns_to_fill].fillna("N")

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value with mode
mode_sex = df["Sex"].mode()[0]
print("Mode of sex:", mode_sex)

mode_race = df["Race"].mode()[0]
print("\nMode of Race:", mode_race)

mode_doi = df["Description of Injury"].mode()[0]
print("\nMode of Description of Injury:", mode_doi)

mode_mod = df["Manner of Death"].mode()[0]
print("\nMode of Manner of Death:", mode_mod)

df["Sex"]=df["Sex"].fillna(df["Sex"].mode()[0])
df["Race"]=df["Race"].fillna(df["Race"].mode()[0])
df["Manner of Death"] = df["Manner of Death"].fillna(df["Manner of Death"].mode()[0])
df["Description of Injury"] = df["Description of Injury"].fillna(df["Description of Injury"].mode()[0])

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing values of "Location if Other"
#if empty and 'Location' = "other", replace missing values with UKNOWN
df.loc[df['Location if Other'].isna() & (df['Location'] == "Other"), 'Location if Other'] = "UNKNOWN"

#fill the rest with none
df['Location if Other']=df['Location if Other'].fillna("None")

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value of Other Significant Conditions
print(df["Other Significant Conditions "].mode([0]))

#replace missing value with substance use
df["Other Significant Conditions "] = df["Other Significant Conditions "].fillna("Substance Use")

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value of "Other" and "Other Opioid"
# Correct approach to avoid chained assignment warning
df.fillna({"Other": "None",
           "Other Opioid": "None"}, inplace=True)

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handling missing values of injury place
#fill missing values with other if residence city != injury city
df.loc[(df['Residence City'].notna()) & (df['Residence City'] != df['Injury City']) & (df['Injury Place'].isna()),'Injury Place'] = 'Other'

#Fill missing value with residence if residence city = injury city
df.loc[(df['Residence City'] == df['Injury City']) & (df['Injury Place'].isna()),'Injury Place'] = 'Residence'

#fill rest of the missing value with mode
df['Injury Place']=df['Injury Place'].fillna(df['Injury Place'].mode()[0])
print("\nMode of Injury Place:", df["Injury Place"].mode()[0])

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle missing value of location
#fill missing values with other if residence city != death city
df.loc[(df['Residence City'].notna()) & (df['Residence City'] != df['Death City']) & (df['Location'].isna()),'Location'] = 'Other'

#Fill missing value with residence if residence city = death city
df.loc[(df['Residence City'] == df['Death City']) & (df['Location'].isna()),'Location'] = 'Residence'

#fill rest of the missing value with mode
df['Location']=df['Location'].fillna(df['Location'].mode()[0])
print("\nMode of Location:", df["Location"].mode()[0])

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handling missing value in residence city
#list of residential location in injury place
residential_places_ip = [
    'Residence', 'residence', 'Residential Building', 'residential building', 'Home', 'house', 'House',
    'Apartment', 'Apartment House', 'Family Residence', "Friend's Residence", "Friend's home",
    "Relative's Home", 'Residential Institution', 'Halfway House', 'Sober house', 'Rest Home',
    'Rehab House', 'Detoxification Center', 'Assisted Living', 'Nursing Home'
]

#if injury place is in residential places, residence city = injury city
df.loc[df['Injury Place'].isin(residential_places_ip), 'Residence City'] = df['Injury City']

#list of residential location in location
residential_places_location = [
    'Residence', "Decedentâ€™s Home", "Decedent's Home", 'Nursing Home', 'Assisted Living', 'Shelter', 'Hospice',
    'Hospice Facility', 'Convalescent Home'
]

#if location is in residential places, residence city = death city
df.loc[df['Location'].isin(residential_places_ip), 'Residence City'] = df['Death City']

#if injury city = death city, fill in missing value with injury city
df.loc[df['Residence City'].isna() & (df['Injury City'] == df['Death City']), 'Residence City'] = df['Injury City']

#fill the rest of the missing value with mode
df['Residence City']=df['Residence City'].fillna(df['Residence City'].mode()[0])
print("\nMode of Residence City:", df["Residence City"].mode()[0])

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handling missing value in injury city
#list of residential location in injury place
residential_places_ip = [
    'Residence', 'residence', 'Residential Building', 'residential building', 'Home', 'house', 'House',
    'Apartment', 'Apartment House', 'Family Residence', "Friend's Residence", "Friend's home",
    "Relative's Home", 'Residential Institution', 'Halfway House', 'Sober house', 'Rest Home',
    'Rehab House', 'Detoxification Center', 'Assisted Living', 'Nursing Home'
]

#if injury place is in residential places, injury city = residence city
df.loc[df['Injury Place'].isin(residential_places_ip), 'Injury City'] = df['Residence City']

#if residence city = death city, fill in missing value with residence city
df.loc[df['Injury City'].isna() & (df['Residence City'] == df['Death City']), 'Injury City'] = df['Residence City']

#fill the rest of the missing value with mode
df['Injury City']=df['Injury City'].fillna(df['Injury City'].mode()[0])
print("\nMode of Injury City:", df["Injury City"].mode()[0])

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handling missing value of death city
#list of residential location in location
residential_places_location = [
    'Residence', "Decedentâ€™s Home", "Decedent's Home", 'Nursing Home', 'Assisted Living', 'Shelter', 'Hospice',
    'Hospice Facility', 'Convalescent Home'
]

#if location is in residential places, death city = residence city
df.loc[df['Location'].isin(residential_places_ip), 'Death City'] = df['Residence City']

#if residence city = injury city, fill in missing value with residence city
df.loc[df['Death City'].isna() & (df['Residence City'] == df['Injury City']), 'Death City'] = df['Residence City']

#fill the rest of the missing value with mode
df['Death City']=df['Death City'].fillna(df['Death City'].mode()[0])
print("\nMode of Death City:", df["Death City"].mode()[0])

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#dictionaries to store the most common values of each city
county_mode = defaultdict(str)
state_mode = defaultdict(str)

#find the most common county and state of each residence city
for city in df["Residence City"].dropna().unique():
    county_mode[city] = df[df["Residence City"] == city]["Residence County"].mode()[0] if not df[df["Residence City"] == city]["Residence County"].mode().empty else None
    state_mode[city] = df[df["Residence City"] == city]["Residence State"].mode()[0] if not df[df["Residence City"] == city]["Residence State"].mode().empty else None

#fill missing values for residence county and state based on residence city
df["Residence County"] = df.apply(lambda row: county_mode[row["Residence City"]] if pd.isna(row["Residence County"]) else row["Residence County"], axis=1)
df["Residence State"] = df.apply(lambda row: state_mode[row["Residence City"]] if pd.isna(row["Residence State"]) else row["Residence State"], axis=1)

#find the most common county and state of each injury city
for city in df["Injury City"].dropna().unique():
    county_mode[city] = df[df["Injury City"] == city]["Injury County"].mode()[0] if not df[df["Injury City"] == city]["Injury County"].mode().empty else None
    state_mode[city] = df[df["Injury City"] == city]["Injury State"].mode()[0] if not df[df["Injury City"] == city]["Injury State"].mode().empty else None

#fill missing values for injury county and state based on injury city
df["Injury County"] = df.apply(lambda row: county_mode[row["Injury City"]] if pd.isna(row["Injury County"]) else row["Injury County"], axis=1)
df["Injury State"] = df.apply(lambda row: state_mode[row["Injury City"]] if pd.isna(row["Injury State"]) else row["Injury State"], axis=1)

#find the most common county and state of each death city
for city in df["Death City"].dropna().unique():
    county_mode[city] = df[df["Death City"] == city]["Death County"].mode()[0] if not df[df["Death City"] == city]["Death County"].mode().empty else None
    state_mode[city] = df[df["Death City"] == city]["Death State"].mode()[0] if not df[df["Death City"] == city]["Death State"].mode().empty else None

#fill missing values for death county and state based on death city
df["Death County"] = df.apply(lambda row: county_mode[row["Death City"]] if pd.isna(row["Death County"]) else row["Death County"], axis=1)
df["Death State"] = df.apply(lambda row: state_mode[row["Death City"]] if pd.isna(row["Death State"]) else row["Death State"], axis=1)

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

##fill in rest of the missing value with mode for residence, injury and death county and state
df['Residence County']=df['Residence County'].fillna(df['Residence County'].mode()[0])
df['Residence State']=df['Residence State'].fillna(df['Residence State'].mode()[0])
df['Injury County']=df['Injury County'].fillna(df['Injury County'].mode()[0])
df['Injury State']=df['Injury State'].fillna(df['Injury State'].mode()[0])
df['Death County']=df['Death County'].fillna(df['Death County'].mode()[0])
df['Death State']=df['Death State'].fillna(df['Death State'].mode()[0])

#recheck missing value
print("\nMissing Values Per Columns:\n", df.isnull().sum())

#handle duplicate values
print("\nDuplicate Values Per Columns:\n", df.duplicated().sum())

#remove whitespace
#remove whitespace in column
df.columns = df.columns.str.replace(" ", "_")

#remove whitespace in rows
df.replace(to_replace=r'\s+', value='_', regex=True, inplace=True)

print(df.head())

#outlier detection
#z-score for age outlier detection
numerical_cols = df.select_dtypes(include=[np.number])

z_scores = np.abs(stats.zscore(numerical_cols))

threshold = 3

outliers = (z_scores > threshold)

df[outliers.any(axis=1)]

"""# Data Transformation"""

#feature scaling for age
scaler = MinMaxScaler()
age_scaled = scaler.fit_transform(df[['Age']])

#label Encoding
#drug type
drug_columns = ['Heroin', 'Heroin_death_certificate_(DC)', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
                'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
                'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
                'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Any_Opioid']

#LabelEncoder
le = LabelEncoder()

for col in drug_columns:
    df[col] = le.fit_transform(df[col])

print(df.head())

#create Drug_Count column
#drug type
drug_columns = ['Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
                'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
                'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
                'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Any_Opioid']

#convert "Other_Opioid" and "Other" to binary
other_opioid_binary= df['Other_Opioid'].apply(lambda x: 1 if isinstance(x, str) and x.strip() != "None" else 0)
other_binary = df['Other'].apply(lambda x: 1 if isinstance(x, str) and x.strip() != "None" else 0)

#create Drug_Count column with sum of row for drugs used
df['Drug_Count'] = df[drug_columns].sum(axis=1) + other_opioid_binary + other_binary

print(df.head())

#creating Age_Group column
bins = [0, 18, 35, 50, df['Age'].max()]
labels = ['0-18', '19-35', '36-50', '51+']
df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)

new_order = ["Date", "Date_Type", "Age", "Age_Group", "Sex", "Race", "Ethnicity",
        "Residence_City", "Residence_County", "Residence_State",
        "Injury_City", "Injury_County", "Injury_State", "Injury_Place",
        "Description_of_Injury", "Death_City", "Death_County", "Death_State",
        "Location", "Location_if_Other", "Cause_of_Death", "Manner_of_Death",
        "Other_Significant_Conditions_", "Heroin", "Heroin_death_certificate_(DC)",
        "Cocaine", "Fentanyl", "Fentanyl_Analogue", "Oxycodone", "Oxymorphone",
        "Ethanol", "Hydrocodone", "Benzodiazepine", "Methadone", "Meth/Amphetamine",
        "Amphet", "Tramad", "Hydromorphone", "Morphine_(Not_Heroin)", "Xylazine",
        "Gabapentin", "Opiate_NOS", "Heroin/Morph/Codeine", "Any_Opioid",
        "Other_Opioid", "Other", "Drug_Count","ResidenceCityGeo", "InjuryCityGeo", "DeathCityGeo"]

df = df[new_order]
print(df.head())

"""# Data Reduction"""

#dropped columns "ResidenceCityGeo", "InjuryCityGeo", "DeathCityGeo"
df = df.copy()  # Ensure df is a separate DataFrame
df.drop(columns=["ResidenceCityGeo", "InjuryCityGeo", "DeathCityGeo"], inplace=True)

print(df.head())

#recheck missing values
print("\nMissing Values Per Columns:\n", df.isnull().sum())

"""# Export Cleaned Dataset"""

#export cleaned dataset
df.to_excel('Accidental_Drug_Related_Deaths_Cleaned.xlsx', index=False)

"""# Visualization"""

#Boxplot for age outlier detection
plt.figure(figsize=(10, 6))
sns.boxplot(x='Age', data=df, color='orange')
plt.title('Outlier Analysis of Age')
plt.xlabel('Age')
plt.show()

#histogram for age distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=20, kde=True, color='orange')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')

# Pie chart for the distribution of Sex
num_sex = df['Sex'].value_counts()
plt.figure(figsize=(8, 8))  # Set the figure size
plt.pie(num_sex, labels=num_sex.index, autopct='%1.1f%%', colors=['lightblue', 'orange'])
plt.title('Distribution of Sex')
plt.show()

plt.figure(figsize=(10, 5))
sns.displot(df, x="Race", height=5, aspect=2, kind="hist", discrete=True)
plt.xticks(rotation=90)
plt.title("Distribution of Race")
plt.xlabel("Race")
plt.ylabel("Count")
plt.show()

#Bar plot of the distribution of ethinicity
#sum of number of ethinicity
ethnicity_counts = df['Ethnicity'].value_counts()

#plotting
plt.figure(figsize=(10, 5))
sns.barplot(x=ethnicity_counts.index, y=ethnicity_counts.values, hue=ethnicity_counts.index, palette="coolwarm", legend=False)
plt.title("Distribution of Ethnicity in Overdose Cases")
plt.xlabel("Ethnicity")
plt.ylabel("Count")
plt.show()

#Countplot for cause of death based on age group
#top5 cause of  death
top_causes = df['Cause_of_Death'].value_counts().nlargest(5).index
filtered_df = df[df['Cause_of_Death'].isin(top_causes)]

#plot countplot
plt.figure(figsize=(12, 6))
sns.countplot(data=filtered_df, x='Age_Group', hue='Cause_of_Death', palette='Set2')
plt.title('Drug Overdose Types by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.legend(title='Cause of Death')
plt.show()

#stacked bar chart for top 5 drug used by age group
#group age and sum by drug used
df['Other_Opioid'] = other_opioid_binary
df['Other'] = other_binary

#drug type
drug_barplot_stacked = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other','Any_Opioid'
]


#top 5 most frequently used drugs
drug_age_group = df.groupby('Age_Group',observed=False)[drug_barplot_stacked].sum()
top_drugs = drug_age_group.sum().nlargest(5).index
drug_age_group_filtered = drug_age_group[top_drugs]

#stacked bar chart
drug_age_group_filtered.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(12, 6))
plt.title('Top 5 Drug Usages by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.legend(title='Drug')
plt.show()

#scatterplot of age vs number of drug involved
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="Age", y="Drug_Count", alpha=0.5, color='blue')
plt.title("Age vs. Number of Drugs Involved in Overdose Cases")
plt.xlabel("Age")
plt.ylabel("Number of Drugs Involved")
plt.show()

#barplot of drug usge frequency
#convert to binary
df['Other_Opioid'] = other_opioid_binary
df['Other'] = other_binary

#drug type
drug_barplot = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other', 'Any_Opioid'
]

#drug usage counts
drug_sum = df[drug_barplot].sum()

#plot
drug_counts = drug_sum.sort_values(ascending=False)
plt.figure(figsize=(12, 5))
sns.barplot(x=drug_counts.index, y=drug_counts.values, hue=drug_counts.index, palette="coolwarm", legend=False)
plt.xticks(rotation=45, ha='right')
plt.title('Drug Usage Frequency in Overdose Cases')
plt.xlabel('Drug')
plt.ylabel('Count')
plt.show()

#heatmap of co-occurance drugs
df['Other_Opioid'] = other_opioid_binary
df['Other'] = other_binary

#drug type
drug_heatmap = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other', 'Any_Opioid'
]

#correlation matrix
drug_corr = df[drug_heatmap].corr()


#plot
plt.figure(figsize=(12, 8))
sns.heatmap(drug_corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Co-occurrence of Drugs in Overdose Cases")
plt.show()

#horizontalbar chart of rarest type of cause of death
#Number of cause of death
cause_of_death_count = df['Cause_of_Death'].value_counts()

#5 least frequent causes
rarest_causes = cause_of_death_count.nsmallest(5)

#plot
plt.figure(figsize=(10, 5))
sns.barplot(x=rarest_causes.values, y=rarest_causes.index, hue=rarest_causes.index, palette="coolwarm", legend=False)
plt.title("Rarest Causes of Death in Overdose Cases")
plt.xlabel("Count")
plt.ylabel("Cause of Death")
plt.show()

df['Other_Opioid'] = other_opioid_binary
df['Other'] = other_binary

#drug type
drug_pairplot = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other', 'Any_Opioid'
]

df['Drug_Count'] = df[drug_pairplot].sum(axis=1)

sns.pairplot(df, vars=['Age', 'Drug_Count'], hue='Sex', diag_kind="kde")
plt.suptitle("Pairplot of Age, Drug Count, and Sex", y=1.02)
plt.show()

"""# Apriori Algoritm"""

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

df['Other_Opioid'] = other_opioid_binary
df['Other'] = other_binary

# Selecting relevant drug-related columns
drug_apriori = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other','Any_Opioid'
]

# Convert numerical indicators (1 for presence, 0 for absence) into transactions
transactions = df[drug_apriori].apply(lambda row: [drug for drug in drug_apriori if row[drug] == 1], axis=1)

# Convert transactions to the format required for Apriori
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_transformed = pd.DataFrame(te_ary, columns=te.columns_)

# Apply Apriori Algorithm to find frequent itemsets
frequent_itemsets = apriori(df_transformed, min_support=0.045, use_colnames=True)

# Extract association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Display results
print("Frequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

!pip install apyori
from apyori import apriori as ap

# Selecting relevant drug-related columns
drug_apriori = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other', 'Any_Opioid'
]


# Convert numerical indicators (1 for presence, 0 for absence) into transactions
transactions = df[drug_apriori].apply(lambda row: [drug for drug in drug_apriori if row[drug] == 1], axis=1)

# **Step 1: Pruning - Remove empty or single-item transactions**
filtered_transactions = [t for t in transactions if len(t) >= 2]

# **Step 2: Apply Apriori Algorithm with Candidate Generation & Pruning**
rd = {"Rule": [], "Support": [], "Confidence": [], "Lift": []}

association_rules = ap(
    filtered_transactions,
    min_support=0.005,  # Minimum support threshold
    min_confidence=0.3,  # Minimum confidence (30%)
    min_lift=3.5,  # Minimum lift (higher means stronger relationships)
    min_length=2  # At least 2 items in rule
)

association_results = list(association_rules)
print("Discovered Results:\n", association_results)
print("Association Types:\n", type(association_results[0]))

# **Step 3: Extract meaningful rules**
for item in association_results:
    pair = item[0]  # Get base item + added item
    items = [x for x in pair]

    if len(items) > 1:  # Ensure rules have at least 2 items
        rd["Rule"].append(f"{items[0]} -> {items[1]}")
        rd["Support"].append(item[1])
        rd["Confidence"].append(item[2][0][2])
        rd["Lift"].append(item[2][0][3])



# **Step 4: Convert results to DataFrame & remove duplicates**
df_rules = pd.DataFrame(rd)
df_rules.drop_duplicates(subset=["Support", "Confidence"], keep="first", inplace=True)
df_rules.reset_index(drop=True, inplace=True)

# **Display Results**
print("\nðŸ” Frequent Itemsets:")
print(df_rules)

# Optional: Save to CSV for documentation
df_rules.to_csv("Apriori_Drug_Association_Results.csv", index=False)

#relevant imports
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori as apmlx, association_rules as rulesmlx


#relevant drug-related columns for association rule mining
acm_drugs = [
    'Heroin', 'Cocaine', 'Fentanyl', 'Fentanyl_Analogue', 'Oxycodone', 'Oxymorphone',
    'Ethanol', 'Hydrocodone', 'Benzodiazepine', 'Methadone', 'Meth/Amphetamine',
    'Amphet', 'Tramad', 'Hydromorphone', 'Morphine_(Not_Heroin)', 'Xylazine',
    'Gabapentin', 'Opiate_NOS', 'Heroin/Morph/Codeine', 'Other_Opioid', 'Other', 'Any_Opioid'
]

# Convert binary drug indicators (1 = present, 0 = absent) into a transaction format
transactions = df[drug_columns].apply(lambda row: [drug for drug in drug_columns if row[drug] == 1], axis=1)

# Remove empty or single-item transactions
filtered_transactions = [t for t in transactions if len(t) >= 2]

# Convert transactions to one-hot encoding
te = TransactionEncoder()
te_ary = te.fit(filtered_transactions).transform(filtered_transactions)
dataset_encoded = pd.DataFrame(te_ary, columns=te.columns_)

# Apply Apriori algorithm to find frequent itemsets
frequent_itemsets = apmlx(dataset_encoded, min_support=0.005, use_colnames=True)

# Generate association rules based on confidence
rules = rulesmlx(frequent_itemsets, metric="confidence", min_threshold=0.3)

# Filter strong rules with high lift
strong_rules = rules[(rules['lift'] >= 3.5) & (rules['confidence'] >= 0.3)]

# Display results
print("\nFrequent Itemsets:")
print(frequent_itemsets.sort_values(by="support", ascending=False))

print("\nStrong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# Save results to CSV
strong_rules.to_csv("Apriori_Drug_Association_Results.csv", index=False)

"""# FP Growth"""

from mlxtend.frequent_patterns import fpgrowth
#An alternative to Apriori, which is often faster.
#min_support=0.045 â†’ Filters itemsets that appear in at least 4.5% of transactions.
fp_results = fpgrowth(dataset_encoded, min_support= 0.045, use_colnames= True)

#Apply rules
#Uses lift as the metric and only keeps rules with lift â‰¥ 1.
overall_results = rulesmlx(fp_results, metric="lift", min_threshold=1)
print("\n\nOverall Results with FP-Growth:\n", overall_results)